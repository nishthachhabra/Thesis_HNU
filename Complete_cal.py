"""
Combined Thesis Metrics Calculation File
"""

import time
import json
import os
from collections import defaultdict
from typing import List, Dict
from datetime import datetime
from difflib import SequenceMatcher
import numpy as np
import pandas as pd
from math import asin, sqrt
from scipy.stats import chi2

# ============================================================================
# MAIN VERIFIER CLASS
# ============================================================================

class ThesisMetricsVerifier:

    def __init__(self):
        self.results = {}

        # ---- ChromaDB (PRIMARY RETRIEVAL SYSTEM) ----
        try:
            from models.query_chromadb import ChromaKnowledgeBaseQuery
            self.kb_query = ChromaKnowledgeBaseQuery(
                persist_directory="data/chromadb"
            )
            print("‚úÖ ChromaDB loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è ChromaDB not available: {e}")
            self.kb_query = None


    # ============================================================================
    # LOAD ALL 21 QUESTIONS
    # ============================================================================

    def load_evaluation_questions(self, json_path: str) -> List[Dict]:
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"Missing file: {json_path}")

        with open(json_path, "r", encoding="utf-8") as f:
            questions = json.load(f)

        print(f"‚úÖ Loaded {len(questions)} evaluation questions")
        return questions


    # ============================================================================
    # RESPONSE LATENCY + RELEVANCE (THESIS SECTION 4.1)
    # ============================================================================

    def measure_response_latency(self, test_queries: List[Dict]) -> Dict:
        timings = defaultdict(list)
        relevance = defaultdict(list)

        for q in test_queries:
            query = q["query"]
            lang = q.get("language", "en")

            total_start = time.perf_counter()

            # --- Preprocessing
            t0 = time.perf_counter()
            _ = query.lower().strip()
            timings["preprocessing"].append((time.perf_counter() - t0) * 1000)

            # --- Vector Retrieval (ChromaDB)
            t0 = time.perf_counter()
            vector_results = []
            if self.kb_query:
                try:
                    vector_results = self.kb_query.query(
                        query_text=query,
                        top_k=5,
                        filter_language=lang
                    )
                except:
                    pass
            timings["vector_retrieval"].append((time.perf_counter() - t0) * 1000)

            # --- Relevance Scoring
            relevance["vector"].append(
                self._relevance_score(query, vector_results)
            )

            # --- LLM Generation (simulated)
            t0 = time.perf_counter()
            time.sleep(1.8)
            timings["llm_generation"].append((time.perf_counter() - t0) * 1000)

            timings["total"].append((time.perf_counter() - total_start) * 1000)

        self.results["latency"] = self._stats(timings)
        self.results["relevance"] = self._stats(relevance)

        return self.results


    # ============================================================================
    # RETRIEVAL PERFORMANCE (THESIS SECTION 4.1)
    # ============================================================================

    def measure_retrieval_performance(self, test_queries: List[Dict]) -> Dict:
        vector_hits = 0
        total = len(test_queries)

        for q in test_queries:
            query = q["query"]
            results = []

            if self.kb_query:
                try:
                    results = self.kb_query.query(query_text=query, top_k=5)
                except:
                    pass

            if results:
                vector_hits += 1

        self.results["retrieval"] = {
            "vector_hit_rate_%": (vector_hits / total) * 100,
            "total_queries": total
        }

        return self.results["retrieval"]


    # ============================================================================
    # USER PREFERENCE ANALYSIS (THESIS SECTION 4.3)
    # ============================================================================

    def analyze_user_preferences(self) -> Dict:
        # Load the Excel file
        file_path = "ThesisResponses(AB)_tones.xlsx"
        df = pd.read_excel(file_path, sheet_name=0)

        # Choices are in columns 1 to 21 (0-indexed 1:22), after Timestamp
        choices = df.iloc[:, 1:22].copy()  # Rows 0+ are data
        choices.columns = [f'Q{i+1}' for i in range(21)]  # Rename for clarity

        # Overall preference statistics
        a_count = (choices == 'A').sum().sum()
        b_count = (choices == 'B').sum().sum()
        total_valid = a_count + b_count

        # Assume majority vote determines prototype (as in thesis)
        prototype = 'A' if a_count > b_count else 'B'
        preference_rate = (a_count / total_valid) if total_valid > 0 else 0.0

        # Cohen's h
        def cohens_h(p):
            if p <= 0 or p >= 1:
                return 0.0
            return 2 * (asin(sqrt(p)) - asin(sqrt(1 - p)))

        h_overall = cohens_h(preference_rate)

        # McNemar test (continuity corrected)
        diff = abs(a_count - b_count) - 1
        chi2_stat = diff**2 / total_valid if total_valid > 0 else 0
        p_value = chi2.sf(chi2_stat, 1) if chi2_stat > 0 else 1.0

        # 95% Confidence Interval (normal approximation)
        p = preference_rate
        se = np.sqrt(p * (1 - p) / total_valid)
        ci_lower = p - 1.96 * se
        ci_upper = p + 1.96 * se

        # Language-specific breakdown
        eng_queries = [1, 2, 8, 9, 10, 11, 12, 13, 14, 15, 17]  # 11 English
        ger_queries = [3, 4, 5, 6, 7, 16, 18, 19, 20, 21]  # 10 German

        eng_idx = [q-1 for q in eng_queries]  # 0-based
        ger_idx = [q-1 for q in ger_queries]

        # English
        eng_a = (choices.iloc[:, eng_idx] == 'A').sum().sum()
        eng_total = choices.iloc[:, eng_idx].notna().sum().sum()
        eng_rate = eng_a / eng_total if eng_total > 0 else 0.0
        eng_h = cohens_h(eng_rate)

        # German
        ger_a = (choices.iloc[:, ger_idx] == 'A').sum().sum()
        ger_total = choices.iloc[:, ger_idx].notna().sum().sum()
        ger_rate = ger_a / ger_total if ger_total > 0 else 0.0
        ger_h = cohens_h(ger_rate)

        abs_bias = abs(eng_rate - ger_rate)

        self.results["user_preferences"] = {
            "total_valid": int(total_valid),
            "prototype_wins": int(a_count),
            "baseline_wins": int(b_count),
            "preference_rate": float(preference_rate),
            "cohens_h": float(h_overall),
            "mcnemar_chi2": float(chi2_stat),
            "p_value": float(p_value),
            "ci_lower": float(ci_lower),
            "ci_upper": float(ci_upper),
            "language_breakdown": {
                "english": {
                    "prefs": int(eng_total),
                    "rate": float(eng_rate),
                    "cohens_h": float(eng_h)
                },
                "german": {
                    "prefs": int(ger_total),
                    "rate": float(ger_rate),
                    "cohens_h": float(ger_h)
                },
                "absolute_bias": float(abs_bias)
            }
        }

        return self.results["user_preferences"]


    # ============================================================================
    # DIMENSION RATINGS ANALYSIS
    # ============================================================================

    def analyze_dimension_ratings(self) -> Dict:
        # Load the Excel file (same as above)
        file_path = "ThesisResponses(AB)_tones.xlsx"
        df = pd.read_excel(file_path, sheet_name=0)

        # Find all rating columns (they follow pattern Qx_Clarity, etc.)
        rating_cols = [c for c in df.columns if any(d in c for d in ['Clarity','Helpfulness','Empathy','Personalization'])]

        if rating_cols:
            ratings = df[rating_cols].copy()
            
            # Compute mean per dimension across all questions
            clarity_mean = ratings.filter(like='Clarity').mean().mean()
            helpful_mean = ratings.filter(like='Helpfulness').mean().mean()
            empathy_mean = ratings.filter(like='Empathy').mean().mean()
            persona_mean = ratings.filter(like='Personalization').mean().mean()
            
            self.results["dimension_ratings"] = {
                "clarity": {
                    "mean": float(clarity_mean),
                    "percentage": float(clarity_mean / 4 * 100)
                },
                "helpfulness": {
                    "mean": float(helpful_mean),
                    "percentage": float(helpful_mean / 4 * 100)
                },
                "empathy": {
                    "mean": float(empathy_mean),
                    "percentage": float(empathy_mean / 4 * 100)
                },
                "personalization": {
                    "mean": float(persona_mean),
                    "percentage": float(persona_mean / 4 * 100)
                }
            }
        else:
            self.results["dimension_ratings"] = {"error": "No rating columns found"}

        return self.results["dimension_ratings"]


    # ============================================================================
    # REPORT GENERATION
    # ============================================================================

    def generate_report(self, path="combined_metrics_verification_report.txt"):
        lines = [
            "=" * 80,
            "HNU CHATBOT ‚Äì COMBINED THESIS METRICS VERIFICATION REPORT",
            f"Generated: {datetime.now()}",
            "=" * 80
        ]

        for k, v in self.results.items():
            lines.append(f"\n--- {k.upper()} ---")
            lines.append(json.dumps(v, indent=2, default=str))

        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"üìÑ Report saved to {path}")


    # ============================================================================
    # HELPERS
    # ============================================================================

    def _relevance_score(self, query: str, results: List) -> float:
        if not results:
            return 0.0

        scores = []
        for r in results[:5]:
            text = str(r)
            sim = SequenceMatcher(None, query.lower(), text.lower()).ratio()
            scores.append(sim * 5)

        return float(np.mean(scores))

    def _stats(self, data: Dict) -> Dict:
        return {
            k: {
                "mean": float(np.mean(v)),
                "std": float(np.std(v)),
                "min": float(np.min(v)),
                "max": float(np.max(v))
            }
            for k, v in data.items() if v
        }


# ============================================================================
# EXECUTION
# ============================================================================

if __name__ == "__main__":
    verifier = ThesisMetricsVerifier()

    # Assume evaluation_questions.json exists; if not, skip latency and retrieval
    try:
        questions = verifier.load_evaluation_questions(
            "evaluation_questions.json"
        )
        verifier.measure_response_latency(questions)
        verifier.measure_retrieval_performance(questions)
    except FileNotFoundError:
        print("‚ö†Ô∏è evaluation_questions.json not found. Skipping latency and retrieval metrics.")

    verifier.analyze_user_preferences()
    verifier.analyze_dimension_ratings()
    verifier.generate_report()